{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Scikit-Learn using Bag of Words\n",
    "\n",
    "We turn text into numerical feature vectors. First, we assign a fixed integer id to each word occurring in any document of the training set (for instance by building a dictionary from words to integer indices) and for each document #i, count the number of occurrences of each word w and store it in X[i, j] as the value of feature #j where j is the index of word w in the dictionary. Second, we tokenize the data and make a dictionary of feature vectors. Third we find occurences of words and change that to frequency because occurences aren't helpful for longer or shorter documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# I have it so all files are in same directory as this file\n",
    "# read the training data\n",
    "df = pd.read_csv('train.csv')\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EAP': 0, 'HPL': 1, 'MWS': 2}\n"
     ]
    }
   ],
   "source": [
    "authors = dict([(auth, idx) for idx, auth in enumerate(df['author'].unique())])\n",
    "print(authors)\n",
    "df['author_id'] = df['author'].apply(lambda x: authors[x])\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split training set into training data set and test data set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.text, df.author_id, test_size=0.13, random_state=42)\n",
    "# X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17033, 23791)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CountVectorizer does text preprocessing, tokenizing, and filtering of stopwords. It is able to build a dictionary of\n",
    "# features and transform documents to feature vectors\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_vect.vocabulary_.get(u'algorithm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17033, 23791)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate term frequency and downscale weights for terms that appear over many documents\n",
    "# tfidf means term frequency, inverse document frequency\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training on the naive bayes classifier with the MultinomialNB which is best for word counts\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'I am not sure to what limit his knowledge may extend.' => 0        EAP\n",
      "1        HPL\n",
      "2        EAP\n",
      "3        MWS\n",
      "4        HPL\n",
      "5        MWS\n",
      "6        EAP\n",
      "7        EAP\n",
      "8        EAP\n",
      "9        MWS\n",
      "10       MWS\n",
      "11       EAP\n",
      "12       HPL\n",
      "13       HPL\n",
      "14       EAP\n",
      "15       MWS\n",
      "16       EAP\n",
      "17       MWS\n",
      "18       EAP\n",
      "19       HPL\n",
      "20       EAP\n",
      "21       HPL\n",
      "22       EAP\n",
      "23       EAP\n",
      "24       EAP\n",
      "25       EAP\n",
      "26       EAP\n",
      "27       EAP\n",
      "28       HPL\n",
      "29       HPL\n",
      "        ... \n",
      "19549    MWS\n",
      "19550    EAP\n",
      "19551    EAP\n",
      "19552    EAP\n",
      "19553    EAP\n",
      "19554    HPL\n",
      "19555    EAP\n",
      "19556    EAP\n",
      "19557    EAP\n",
      "19558    EAP\n",
      "19559    HPL\n",
      "19560    EAP\n",
      "19561    HPL\n",
      "19562    EAP\n",
      "19563    MWS\n",
      "19564    EAP\n",
      "19565    EAP\n",
      "19566    MWS\n",
      "19567    EAP\n",
      "19568    EAP\n",
      "19569    MWS\n",
      "19570    MWS\n",
      "19571    HPL\n",
      "19572    EAP\n",
      "19573    MWS\n",
      "19574    EAP\n",
      "19575    EAP\n",
      "19576    EAP\n",
      "19577    EAP\n",
      "19578    HPL\n",
      "Name: author, Length: 19579, dtype: object\n",
      "'For I am Iranon, who was a Prince in Aira.' => 0        EAP\n",
      "1        HPL\n",
      "2        EAP\n",
      "3        MWS\n",
      "4        HPL\n",
      "5        MWS\n",
      "6        EAP\n",
      "7        EAP\n",
      "8        EAP\n",
      "9        MWS\n",
      "10       MWS\n",
      "11       EAP\n",
      "12       HPL\n",
      "13       HPL\n",
      "14       EAP\n",
      "15       MWS\n",
      "16       EAP\n",
      "17       MWS\n",
      "18       EAP\n",
      "19       HPL\n",
      "20       EAP\n",
      "21       HPL\n",
      "22       EAP\n",
      "23       EAP\n",
      "24       EAP\n",
      "25       EAP\n",
      "26       EAP\n",
      "27       EAP\n",
      "28       HPL\n",
      "29       HPL\n",
      "        ... \n",
      "19549    MWS\n",
      "19550    EAP\n",
      "19551    EAP\n",
      "19552    EAP\n",
      "19553    EAP\n",
      "19554    HPL\n",
      "19555    EAP\n",
      "19556    EAP\n",
      "19557    EAP\n",
      "19558    EAP\n",
      "19559    HPL\n",
      "19560    EAP\n",
      "19561    HPL\n",
      "19562    EAP\n",
      "19563    MWS\n",
      "19564    EAP\n",
      "19565    EAP\n",
      "19566    MWS\n",
      "19567    EAP\n",
      "19568    EAP\n",
      "19569    MWS\n",
      "19570    MWS\n",
      "19571    HPL\n",
      "19572    EAP\n",
      "19573    MWS\n",
      "19574    EAP\n",
      "19575    EAP\n",
      "19576    EAP\n",
      "19577    EAP\n",
      "19578    HPL\n",
      "Name: author, Length: 19579, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# to try to predict the outcome on a new document we need to extract the features using almost the same feature \n",
    "# extracting chain as before. The difference is that we call transform instead of fit_transform on the transformers, \n",
    "# since they have already been fit to the training set\n",
    "docs_new = ['I am not sure to what limit his knowledge may extend.', 'For I am Iranon, who was a Prince in Aira.']\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, df.author))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# building a pipeline, vectorizer > transformer > classifier, acts as a compound classifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8142183817753339"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the test set for accuracy of the model\n",
    "import numpy as np\n",
    "predicted = text_clf.predict(X_test)\n",
    "np.mean(predicted == y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82560879811468968"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using a linear support vector machine\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                           alpha=1e-4, random_state=42,\n",
    "                                           max_iter=5, tol=None)),\n",
    "])\n",
    "text_clf.fit(X_train, y_train)  \n",
    "\n",
    "predicted = text_clf.predict(X_test)\n",
    "np.mean(predicted == y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        EAP       0.80      0.86      0.83      1005\n",
      "        HPL       0.83      0.81      0.82       690\n",
      "        EAP       0.86      0.80      0.83       851\n",
      "\n",
      "avg / total       0.83      0.83      0.83      2546\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robert/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1428: UserWarning: labels size, 3, does not match size of target_names, 19579\n",
      "  .format(len(labels), len(target_names))\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, predicted,\n",
    "    target_names=df.author))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[867,  69,  69],\n",
       "       [ 95, 558,  37],\n",
       "       [125,  49, 677]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "models = [('MultiNB', MultinomialNB(alpha=0.03)),\n",
    "          ('Calibrated MultiNB', CalibratedClassifierCV(\n",
    "              MultinomialNB(alpha=0.03), method='isotonic')),\n",
    "          ('Calibrated BernoulliNB', CalibratedClassifierCV(\n",
    "              BernoulliNB(alpha=0.03), method='isotonic')),\n",
    "          ('Calibrated Huber', CalibratedClassifierCV(\n",
    "              SGDClassifier(loss='modified_huber', alpha=1e-4,\n",
    "                            max_iter=10000, tol=1e-4), method='sigmoid')),\n",
    "          ('Logit', LogisticRegression(C=30))]\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "vectorizer=TfidfVectorizer(token_pattern=r'\\w{1,}', sublinear_tf=True, ngram_range=(1,2))\n",
    "clf = VotingClassifier(models, voting='soft', weights=[3,3,3,1,1])\n",
    "X_train = vectorizer.fit_transform(train.text.values)\n",
    "authors = ['MWS','EAP','HPL']\n",
    "y_train = train.author.apply(authors.index).values\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "test = pd.read_csv('test.csv', index_col=0)\n",
    "X_test = vectorizer.transform(test.text.values)\n",
    "results = clf.predict_proba(X_test)\n",
    "pd.DataFrame(results, index=test.index, columns=authors).to_csv('scikit_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Keras for categorization using feature extraction\n",
    "\n",
    "This model is a bit different in that it deletes rare words to prevent overfitting, it doesn't remove stopwords because an author may use them in unique ways, the same goes for stemming and lowercase, it cuts long sentences and separates punctuation as some punctuation may be unique from author to author. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.layers import Dense, GlobalAveragePooling1D, Embedding\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "a2c = {'EAP': 0, 'HPL' : 1, 'MWS' : 2}\n",
    "y = np.array([a2c[a] for a in df.author])\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c EAP   HPL   MWS   \n",
      "B 835 533 395 \n",
      "z 634 529 400 \n",
      "H 864 741 669 \n",
      "u 26311 19519 21025 \n",
      "I 4846 3480 4917 \n",
      "x 1951 1061 1267 \n",
      "ê 28 2 0 \n",
      "Υ 0 1 0 \n",
      "ἶ 0 2 0 \n",
      "ü 1 5 0 \n",
      "M 1065 645 415 \n",
      "O 414 503 282 \n",
      "m 22792 17622 20471 \n",
      "ï 0 7 0 \n",
      "ô 8 0 0 \n",
      "? 510 169 419 \n",
      "\" 2987 513 1469 \n",
      ". 8406 5908 5761 \n",
      "ñ 0 7 0 \n",
      "α 0 2 0 \n",
      "P 442 320 365 \n",
      "b 13245 10636 9611 \n",
      "p 17422 10965 12361 \n",
      "d 36862 33366 35315 \n",
      "â 6 0 0 \n",
      "T 2217 1583 1230 \n",
      "w 17507 15554 16062 \n",
      "v 9624 6529 7948 \n",
      "D 491 334 227 \n",
      "V 156 67 57 \n",
      "k 4277 5204 3707 \n",
      "g 16088 14951 12601 \n",
      "é 47 15 0 \n",
      "à 10 0 0 \n",
      "K 86 176 35 \n",
      "Ο 0 3 0 \n",
      "Æ 1 4 0 \n",
      "e 114885 88259 97515 \n",
      "ö 16 3 0 \n",
      "G 313 318 246 \n",
      "ç 1 0 0 \n",
      "ä 1 6 0 \n",
      "h 51580 42770 43738 \n",
      "; 1354 1143 2662 \n",
      "q 1030 779 677 \n",
      "E 435 281 445 \n",
      "Ν 0 1 0 \n",
      "F 383 269 232 \n",
      "C 395 439 308 \n",
      "i 60952 44250 46080 \n",
      "N 411 345 204 \n",
      "æ 36 10 0 \n",
      "Π 0 1 0 \n",
      "î 1 0 0 \n",
      "X 17 5 4 \n",
      "W 739 732 681 \n",
      "s 53841 43915 45962 \n",
      "Z 23 51 2 \n",
      "J 164 210 66 \n",
      "L 458 249 307 \n",
      "ë 0 12 0 \n",
      "Å 0 1 0 \n",
      "a 68525 56815 55274 \n",
      ", 17594 8581 12045 \n",
      "o 67145 50996 53386 \n",
      ": 176 47 339 \n",
      "r 51221 40590 44042 \n",
      "y 17001 12534 14877 \n",
      "A 1258 1167 943 \n",
      "c 24127 18338 17911 \n",
      "R 258 237 385 \n",
      "S 729 841 578 \n",
      "t 82426 62235 63142 \n",
      "' 1334 1710 476 \n",
      "Σ 0 1 0 \n",
      "n 62636 50879 50291 \n",
      "U 166 94 46 \n",
      "Q 21 10 7 \n",
      "l 35371 30273 27819 \n",
      "j 683 424 682 \n",
      "Y 282 111 234 \n",
      "è 15 0 0 \n",
      "f 22354 16272 18351 \n",
      "δ 0 2 0 \n"
     ]
    }
   ],
   "source": [
    "# checks character distribution per author\n",
    "counter = {name : defaultdict(int) for name in set(df.author)}\n",
    "for (text, author) in zip(df.text, df.author):\n",
    "    text = text.replace(' ', '')\n",
    "    for c in text:\n",
    "        counter[author][c] += 1\n",
    "\n",
    "chars = set()\n",
    "for v in counter.values():\n",
    "    chars |= v.keys()\n",
    "    \n",
    "names = [author for author in counter.keys()]\n",
    "\n",
    "print('c ', end='')\n",
    "for n in names:\n",
    "    print(n, end='   ')\n",
    "print()\n",
    "for c in chars:    \n",
    "    print(c, end=' ')\n",
    "    for n in names:\n",
    "        print(counter[n][c], end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some authors use unique characters and authors use punctuation differently like EAP uses commas more and MWS uses a lot\n",
    "less apostrophes so she probably uses a lot less contractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function that separates punctuation from words\n",
    "def preprocess(text):\n",
    "    text = text.replace(\"' \", \" ' \")\n",
    "    signs = set(',.:;\"?!')\n",
    "    prods = set(text) & signs\n",
    "    if not prods:\n",
    "        return text\n",
    "\n",
    "    for sign in prods:\n",
    "        text = text.replace(sign, ' {} '.format(sign) )\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_docs(df, n_gram_max=2):\n",
    "    def add_ngram(q, n_gram_max):\n",
    "            ngrams = []\n",
    "            for n in range(2, n_gram_max+1):\n",
    "                for w_index in range(len(q)-n+1):\n",
    "                    ngrams.append('--'.join(q[w_index:w_index+n]))\n",
    "            return q + ngrams\n",
    "        \n",
    "    docs = []\n",
    "    for doc in df.text:\n",
    "        doc = preprocess(doc).split()\n",
    "        docs.append(' '.join(add_ngram(doc, n_gram_max)))\n",
    "    \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# removes lower frequency words\n",
    "# cuts documents that have more than 256 words\n",
    "min_count = 2\n",
    "\n",
    "docs = create_docs(df)\n",
    "tokenizer = Tokenizer(lower=False, filters='')\n",
    "tokenizer.fit_on_texts(docs)\n",
    "num_words = sum([1 for _, v in tokenizer.word_counts.items() if v >= min_count])\n",
    "\n",
    "tokenizer = Tokenizer(num_words=num_words, lower=False, filters='')\n",
    "tokenizer.fit_on_texts(docs)\n",
    "docs = tokenizer.texts_to_sequences(docs)\n",
    "\n",
    "maxlen = 256\n",
    "\n",
    "docs = pad_sequences(sequences=docs, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dimension of a word vector is \n",
    "input_dim = np.max(docs) + 1\n",
    "embedding_dims = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function that creates model with Adam optimizer\n",
    "def create_model(embedding_dims=20, optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=input_dim, output_dim=embedding_dims))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6713 samples, validate on 1679 samples\n",
      "Epoch 1/25\n",
      "6713/6713 [==============================] - 1s - loss: 1.0850 - acc: 0.4038 - val_loss: 1.0788 - val_acc: 0.3996\n",
      "Epoch 2/25\n",
      "6713/6713 [==============================] - 1s - loss: 1.0547 - acc: 0.4268 - val_loss: 1.0384 - val_acc: 0.5021\n",
      "Epoch 3/25\n",
      "6713/6713 [==============================] - 1s - loss: 0.9863 - acc: 0.5634 - val_loss: 0.9690 - val_acc: 0.5456\n",
      "Epoch 4/25\n",
      "6713/6713 [==============================] - 1s - loss: 0.8879 - acc: 0.7323 - val_loss: 0.8841 - val_acc: 0.7576\n",
      "Epoch 5/25\n",
      "6713/6713 [==============================] - 1s - loss: 0.7849 - acc: 0.8157 - val_loss: 0.8092 - val_acc: 0.7445\n",
      "Epoch 6/25\n",
      "6713/6713 [==============================] - 1s - loss: 0.6950 - acc: 0.8515 - val_loss: 0.7457 - val_acc: 0.7707\n",
      "Epoch 7/25\n",
      "6713/6713 [==============================] - 1s - loss: 0.6209 - acc: 0.8716 - val_loss: 0.6944 - val_acc: 0.7999\n",
      "Epoch 8/25\n",
      "6713/6713 [==============================] - 1s - loss: 0.5622 - acc: 0.8895 - val_loss: 0.6509 - val_acc: 0.8148\n",
      "Epoch 9/25\n",
      "6713/6713 [==============================] - 1s - loss: 0.5142 - acc: 0.9030 - val_loss: 0.6147 - val_acc: 0.8273\n",
      "Epoch 10/25\n",
      "6713/6713 [==============================] - 1s - loss: 0.4753 - acc: 0.9197 - val_loss: 0.5905 - val_acc: 0.8303\n",
      "Epoch 11/25\n",
      "6713/6713 [==============================] - 1s - loss: 0.4439 - acc: 0.9278 - val_loss: 0.5659 - val_acc: 0.8338\n",
      "Epoch 12/25\n",
      "6713/6713 [==============================] - 1s - loss: 0.4186 - acc: 0.9345 - val_loss: 0.5424 - val_acc: 0.8481\n",
      "Epoch 13/25\n",
      "6713/6713 [==============================] - 1s - loss: 0.3977 - acc: 0.9438 - val_loss: 0.5280 - val_acc: 0.8523\n",
      "Epoch 14/25\n",
      "6713/6713 [==============================] - 1s - loss: 0.3806 - acc: 0.9508 - val_loss: 0.5105 - val_acc: 0.8594\n",
      "Epoch 15/25\n",
      "6713/6713 [==============================] - 1s - loss: 0.3668 - acc: 0.9547 - val_loss: 0.4973 - val_acc: 0.8672\n",
      "Epoch 16/25\n",
      "6713/6713 [==============================] - 1s - loss: 0.3556 - acc: 0.9629 - val_loss: 0.4901 - val_acc: 0.8666\n",
      "Epoch 17/25\n",
      "6713/6713 [==============================] - 1s - loss: 0.3464 - acc: 0.9662 - val_loss: 0.4777 - val_acc: 0.8749\n",
      "Epoch 18/25\n",
      "6713/6713 [==============================] - 1s - loss: 0.3389 - acc: 0.9720 - val_loss: 0.4692 - val_acc: 0.8791\n",
      "Epoch 19/25\n",
      "6713/6713 [==============================] - 1s - loss: 0.3330 - acc: 0.9732 - val_loss: 0.4622 - val_acc: 0.8797\n",
      "Epoch 20/25\n",
      "6713/6713 [==============================] - 1s - loss: 0.3280 - acc: 0.9778 - val_loss: 0.4568 - val_acc: 0.8821\n",
      "Epoch 21/25\n",
      "6713/6713 [==============================] - 1s - loss: 0.3240 - acc: 0.9791 - val_loss: 0.4514 - val_acc: 0.8833\n",
      "Epoch 22/25\n",
      "6713/6713 [==============================] - 1s - loss: 0.3208 - acc: 0.9824 - val_loss: 0.4466 - val_acc: 0.8833\n",
      "Epoch 23/25\n",
      "6713/6713 [==============================] - 1s - loss: 0.3181 - acc: 0.9830 - val_loss: 0.4419 - val_acc: 0.8868\n",
      "Epoch 24/25\n",
      "6713/6713 [==============================] - 1s - loss: 0.3159 - acc: 0.9857 - val_loss: 0.4394 - val_acc: 0.8839\n",
      "Epoch 25/25\n",
      "6713/6713 [==============================] - 1s - loss: 0.3142 - acc: 0.9869 - val_loss: 0.4367 - val_acc: 0.8886\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "epochs = 25\n",
    "x_train, x_test, y_train, y_test = train_test_split(docs, y, test_size=0.2)\n",
    "\n",
    "model = create_model()\n",
    "hist = model.fit(x_train, y_train,\n",
    "                 batch_size=16,\n",
    "                 validation_data=(x_test, y_test),\n",
    "                 epochs=epochs,\n",
    "                 callbacks=[EarlyStopping(patience=2, monitor='val_loss')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6272/8392 [=====================>........] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "# uses model to predict the author of the test data set\n",
    "test_df = pd.read_csv('test.csv')\n",
    "docs = create_docs(test_df)\n",
    "docs = tokenizer.texts_to_sequences(docs)\n",
    "docs = pad_sequences(sequences=docs, maxlen=maxlen)\n",
    "y = model.predict_proba(docs)\n",
    "\n",
    "result = pd.read_csv('sample_submission.csv')\n",
    "for a, i in a2c.items():\n",
    "    result[a] = y[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result.to_csv('keras_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Convolutional Network for Spooky Author ID1\n",
    "\n",
    "Trying to get Keras to work with both my GTX 1070s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/cpu:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 2111008048821374629\n",
      ", name: \"/gpu:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 71761920\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 2593259007318933880\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0\"\n",
      ", name: \"/gpu:1\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 7984057549\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 5962234433982548780\n",
      "physical_device_desc: \"device: 1, name: GeForce GTX 1070, pci bus id: 0000:02:00.0\"\n",
      "]\n",
      "[[ 22.  28.]\n",
      " [ 49.  64.]]\n"
     ]
    }
   ],
   "source": [
    "# Definitions\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# tensorflow settings to activate gpu\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "\n",
    "BASE_DIR = '../data'\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B')\n",
    "TEXT_DATA_DIR = os.path.join(BASE_DIR, 'SpookyData')\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "import tensorflow as tf\n",
    "# Creates a graph.\n",
    "a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "c = tf.matmul(a, b)\n",
    "# Creates a session with log_device_placement set to True.\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "# Runs the op.\n",
    "print(sess.run(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read the training data\n",
    "df = pd.read_csv('train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EAP': 0, 'HPL': 1, 'MWS': 2}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>author_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS   \n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL   \n",
       "\n",
       "   author_id  \n",
       "0          0  \n",
       "1          1  \n",
       "2          0  \n",
       "3          2  \n",
       "4          1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a list of classifications and generate numeric \n",
    "#  values for each class.  put the numeric class back \n",
    "#  on to the data frame.\n",
    "authors = dict([(auth, idx) for idx, auth in enumerate(df['author'].unique())])\n",
    "print(authors)\n",
    "df['author_id'] = df['author'].apply(lambda x: authors[x])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19579\n",
      "[26, 2945, 143, 1372, 22, 36, 294, 2, 7451, 1, 2440, 2, 10, 4556, 16, 6, 79, 179, 48, 4245, 3, 295, 4, 1, 249, 1943, 6, 326, 74, 134, 123, 891, 2, 1, 313, 39, 1438, 4928, 98, 1, 430]\n",
      "Found 25943 unique tokens before stopwords removal.\n",
      "[('the', 1), ('of', 2), ('and', 3), ('to', 4), ('a', 5)]\n",
      "Found 25808 unique tokens after stopwords removal.\n",
      "Shape of data tensor: (19579, 1000)\n",
      "Shape of label tensor: (19579, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop stop words\n",
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "# now we will use the text and author_id fields to train a classifier.\n",
    "#  We have to: \n",
    "#  1. Get the sentences, \n",
    "sents = df['text'].tolist()\n",
    "labels = df['author_id'].tolist()\n",
    "#  2. Tokenize each sentence, \n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(sents)\n",
    "sequences = tokenizer.texts_to_sequences(sents)\n",
    "print(len(sequences))\n",
    "print(sequences[0])\n",
    "##    Get a vector of unique terms here\n",
    "print('Found %s unique tokens before stopwords removal.' % len(tokenizer.word_index))\n",
    "print([w for w in tokenizer.word_index.items()][:5])\n",
    "word_index = dict([(w,i) for w,i in tokenizer.word_index.items() if w not in stops])\n",
    "print('Found %s unique tokens after stopwords removal.' % len(word_index))\n",
    "\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]\n",
    "y_val[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "#  3. Load embeddings\n",
    "embeddings_index = {}\n",
    "f = open('glove.6B.300d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2092\n"
     ]
    }
   ],
   "source": [
    "#  4. Create the Embedding matrix for the training set\n",
    "num_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "unk = []\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        unk.append(word)\n",
    "print(len(unk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`multi_gpu_model` is only available with the TensorFlow backend.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a31b062407b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_gpu_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m model.compile(loss='mean_squared_logarithmic_error',\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/utils/training_utils.py\u001b[0m in \u001b[0;36mmulti_gpu_model\u001b[0;34m(model, gpus)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'tensorflow'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         raise ValueError('`multi_gpu_model` is only available '\n\u001b[0m\u001b[1;32m     87\u001b[0m                          'with the TensorFlow backend.')\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgpus\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: `multi_gpu_model` is only available with the TensorFlow backend."
     ]
    }
   ],
   "source": [
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "\n",
    "from keras.utils.training_utils import multi_gpu_model\n",
    "\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "\n",
    "print('Training model.')\n",
    "\n",
    "# train a 1D convnet with global maxpooling\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "#x = MaxPooling1D()(x)\n",
    "#x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(.5)(x)\n",
    "preds = Dense(len(authors), activation='softmax')(x)\n",
    "rms = RMSprop(lr=0.003)\n",
    "model = Model(sequence_input, preds)\n",
    "\n",
    "model = multi_gpu_model(model, gpus=2)\n",
    "\n",
    "model.compile(loss='mean_squared_logarithmic_error',\n",
    "              optimizer=rms, #'rmsprop',\n",
    "              metrics=['acc'])\n",
    "#model.compile(loss='categorical_crossentropy',\n",
    "#              optimizer=rms, #'rmsprop',\n",
    "#              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=100,\n",
    "          epochs=50,\n",
    "          validation_data=(x_val, y_val))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
